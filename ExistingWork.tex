\chapter{相关工作}
\rhead{相关工作}
%summary
迄今为止，研究人员已经提出了大量的计算方法来解决这个问题。这些方法可以粗略的归为两类：基于序列相似度和基于组成成分。


\section{基于序列相似度的方法}
基于相似度的方法首先将元基因组序列比对到参考基因组，然后根据比对结果，对序列进行归类。
其中一个典型的方法是MEGAN\cite{huson2009methods}。
显然，如果没有参考基因组，该方法鞭长莫及。
\section{基于组成成分的监督学习方法}
%supervsied
基于组成成分的方法通常直接从核苷酸序列中抽取序列特征，包括寡核苷酸的频率
GC含量，密码子的使用等等。
到现在为止，研究人员已经提出了很多基于组成成分的监督学习方法对序列进行归类，包括SVM~\cite{mchardy2006accurate}，朴素贝叶斯~\cite{stark2010mltreemap}，KNN~\cite{diaz2009tacoa}，Interpolated Markov Model~\cite{brady2009phymm} 等。
然而这些方法的性能仍然在很大程度上依赖于作为训练样本的基因组。
\section{基于组成成分的非监督学习方法}
%unsupervised
为了克服这些方法的缺点，研究人员又提出非监督学习方法来处理未知物种的元基因组序列。
\subsection{AbundanceBin}
Wu等人~\cite{wu2011novel}提出了一种称为AbundanceBin的方法。该方法从序列中抽取$k$-mer特征，并利用$k$-mer覆盖率进行归类，以达到区分物种丰度比差异明显的数据的效果。
具体的，它首先统计序列中$k$-mer词频，定义第$i$类的基因组大小和丰度水平为$g_{i}$, $\lambda_i$,定义类的个数为$S$，因为它还提供一种
自动决定物种数量的递归归类模式。

定义$\theta$=\{S,~g,~$\lambda_i$\}, 归类算法的目标是最大化观测到的词频向量，和参数$\theta$组成的联合概率的对数值$logP(x, \theta)$。
隐变量是每个$k$-mer所属类名。随后，采用期望最大化算法(Expectation-Maximization)算法来解决这个优化问题。

AbundanceBin算法如下：
\begin{enumerate}
\item 初始化三个隐变量，丰度值$\lambda_i$=10， 基因组大小为$g_i$=1000000。
\item 根据泊松分布，计算第$j$个$k$-mer $w_j$属于第i类的概率，如公式\ref{eq:0}:\\
\begin{equation}\label{eq:0}
P(w_j \epsilon s_i\mid n(w_j)) = \frac{g_i}{\sum_{m=1}^{S} g_m(\frac{\lambda_m}{\lambda_i})^{n(w_j)} e^{(\lambda_i-\lambda_m)}}
\end{equation}
\item 由公式\ref{eq:1}、~\ref{eq:2}计算$g_i$,$\lambda_i$:\\
\begin{equation}\label{eq:1}
g_i=\sum_{j=1}^{w}P(w_j\epsilon s_i\mid n(w_j))
\end{equation}
\begin{equation}\label{eq:2}
\lambda_i= \frac{\sum_{j=1}^{W}n(w_j)P(w_j\epsilon s_i\mid n(w_j))}{g_i}
\end{equation}

\item 迭代2,3步，使得参数收敛或者迭代次数达到一定次数。
\end{enumerate}
EM算法收敛之后，按照公式\ref{eq:3},我们可以估计每个序列被归类为每个类的概率，然后依据这一值便可以做出归类的决策，如公式\ref{eq:3}

\begin{equation}\label{eq:3}
P(r_k\epsilon s_i)=\frac{\prod_{w_j\epsilon r_k}P(w_j \epsilon s_i \mid n(w_j))}{\sum_{s_i \epsilon S}(\prod_{w_j\epsilon r_k}P(w_j \epsilon s_i \mid n(w_j)))}
\end{equation}

其中$r_k$ 是第$k$个序列，$w_j$是$r_k$中的第$j$个$k$-mer, $s_i$ 是任意一个类。一个序列将会被归类为所有类中有最高概率的类。

然而当数据集的物种丰度比相同时，AbundanceBin表现差强人意。正如文中~\cite{wu2011novel}所说，需要和MetaCluster 1.0\cite{yang2010metacluster} 结合使用，可以取得较好的归类效果。

\subsection{MetaCluster 3.0}

AbundanceBin只能处理物种丰度比均匀的数据集，对于丰度比差异较大的数据集，效果差强人意。
为了处理丰度比均匀以及差异明显都存在的数据集，Leung等人提出了MetaCluster 3.0~\cite{leung2011robust}方法。

MetaCluster 3.0具体算法如下：
\begin{enumerate}
\item 根据输入序列，计算$k$-mer分布。根据已有文献的结论，$k$=4~是$k$ ~=~2到7里面效果最佳的\cite{chor2009genomic, zhou2008barcodes}。
\item 距离度量采用Spearman Footrule distance\cite{diaconis1977spearman}，这是因为Spearman distance不依赖于数量级，对于有较大值的$k$-mer 不受太大的影响~\cite{leung2011robust}。
\item 采用$K$-median算法，对原始数据进行自上而下的归类，采用Spearman Footrule Distance距离进行度量。
    基于公式\ref{eq:4}，$K$-means算法将样本归类到离它最近的类：
\begin{equation}\label{eq:4}
    MinE=\sum_{i=1}^{k'}\sum_{A\epsilon C_i}dist_s (A, c_i)
\end{equation}
\item 计算每两个类之间的类内距离，如果小于给定的阈值，则归并为一个类。
\end{enumerate}

MetaCluster 3.0 在序列长为1000bp的均匀和不均匀物种上的表现都优于AbundanceBin。但是MetaCluster 3.0不适合处理短序列。


\subsection{MetaCluster 4.0}
为了解决短序列归类的问题，Wang等人提出了MetaCluster 4.0~\cite{wang2012metacluster1}。 该方法基于序列重叠性，将长度小于500bp 的短序列归为一类进行归类。

MetaCluster 4.0算法如下：
\begin{enumerate}
\item 对原始的序列进行概率归类。由于序列较短，归类的时候可能不利于提取特征，因此先将短序列装配成长的``虚拟重叠群``(virtual contig)。
该做法是基于这个结果：不同物种基因组中具有相同的长$w$-mer的概率是非常低的，(来自同一个家族或者属的序列有相同35-mer的概率$<$0.03\% 和$<$0.22\%)

这个过程同时还能减少不均匀丰度比数据集对归类产生的影响。

\item 估计$k$-mer分布。基于之前的研究成果，此处$k$值依然取4。由于每一组序列之间有很多重叠区域，因此不能直接计算所有$k$-mer的频率作为特征。
该方法设置$T$值作为阈值，将出现次数$>=T$的$k$-mer组成$k$-mer分布。

\item 采用和MetaCluster 3.0类似的方法，进行归类，但是由于MetaCluster 3.0先聚成许多小类，然后归并，可能出现估计的物种数不准确的情况，
因此MetaCluster 4.0对于归类的个数进行二分，进而确定最终的类个数。
\end{enumerate}


\subsection{MetaCluster 5.0}
为了能够处理低丰度短序列数据，Wang等人随后提出了MetaCluster 5.0~\cite{wang2012metacluster2}。该算法基于下述的三个观察现象：
\begin{enumerate}
\item  采样自一个基因组的序列的$k$-mer的频率通常是和数据集中该基因组序列的丰度指是线性正相关的。
\item  较长的$w$-mer 通常在一个基因组中是不太可能出现多次的\cite{wang2012metacluster1}。
\item  来自一个或者相似的基因组中单个长序列中的短$q$-mer的分布是相似的。
\end{enumerate}

MetaCluster 5.0的算法如下所示：

\begin{enumerate}
\item \textbf{第一轮归类}，输入是所有的序列。

\item 第一步，过滤掉一些低丰度和极低丰度的序列。\\
由于之前的很多算法经常难以处理同时混合有高丰度和低丰度数据，因此该算法首先将低丰度和高丰度数据区分开，
基于观察1，我们可以根据一个序列的$k$-mer频率值来区分高丰度序列和低丰度序列。具体的，定义阈值$T$，对于一个序列，所有它的$k$-mer 频率都$<=T$，则将它归类为低丰度和极低丰度序列，然后将其过滤掉，
放在第二轮归类处理。此处$T$=4。\\

\item 第二步，根据两个序列是否有重叠的$w$-mer将序列组装为``虚拟重叠群''。

根据观察2，我们可以知道，来自不同物种的序列具有公共$w$-mer(当$w$值较大时)的概率是非常小的\cite{wang2012metacluster1}, 此处取$w$=36。 该步处理与MetaCluster 4.0 算法不同之处在于，生成的``虚拟重叠群'' 长度较长，此处可能依然会有低丰度数据混入，因此将``虚拟重叠群'' 长度低于1000bp 的看成是低丰度数据并过滤到第二轮来处理。此外，MetaCluster 4.0 只是把有重叠$w$-mer 的序列归在一组，没有进行组装的过程，该算法进行了组装工作，便于后面直接计算$k$-mer 分布进行归类。\\

\item 第三步，对组装后的``虚拟重叠群''进行最后的归类。

具体的，抽取$k$-mer特征，基于Spearman Distance用$k$-means算法进行归类。此处由于序列丰度较高，$k$=5。\\

\item \textbf{第二轮归类}，输入是第一轮过滤掉的序列。

\item 第一步，和第一轮归类的第一步相似，但是由于序列都为低丰度和极低丰度数据，丰度降低，因此$T$值也降低为2，也即序列里全部$k$-mer 频率都是1 的序列被认为是极低丰度，被丢弃，不进行后续处理，以此来节约算法的时间和空间代价。

\item 第二步，和第一轮归类的第二步相似，同样由于序列丰度降低，因此$w$值降低，此处设为22。

\item 第三步，和第一轮归类的第三步相似，对组装的``虚拟重叠群'' 进行归类，抽取$k$-mer特征进行归类，此处序列丰度降低，$k$=4。
\end{enumerate}

最后MetaCluster 5.0对高丰度数据进行归类，低丰度数据也进行归类，极低丰度数据不归类，直接丢弃。

这一系列的MetaCluster算法可以自动确定簇的数目，这对于真实数据中绝大多数序列的物种名未知时是至关重要的。
\iffalse
\subsection{MetaCluster-TA}
2014年 APBC会议上，Wang Yi等研究人员提出了 MetaCluster-TA~\cite{wang2014metacluster}-----通过装配和归类来进行注释的序列分类注释工具。它先将短序列装配成长的“虚拟重叠群”，并
然后应用类似于MetaCluster 5.0的方法来对这些重叠群和序列进行归类，
并最后将所有的簇进行归类。
\fi
\subsection{MCluster}
最近廖瑞琪还提出了一种新的非监督方法MCluster~\cite{liao2014new} 对元基因组序列进行归类。他的创新点在于归类算法在$k$-means算法的基础上，有自动加权机制，每个特征对于每个类都有一个权重，每个类里面的两个样本之间的距离不是直接每一维距离累加，而是带权的累加。通过实验可以验证，这种加权的归类算法可以取得更好的归类效果。具体而言，在长序列上，MCluster取得了比AbundanceBin以及MetaCluster 3.0明显更优的性能，在短序列上，和MetaCluster 5.0 相比，MCluster 得到了更高的$Sensitivity$以及相比之下更加稳定的$F1$值。
具体算法流程如下：

\begin{enumerate}
\item 基于$n$-gram模型，从序列中抽取$k$-mer特征。
\item  采用SKWIC算法\cite{frigui2004simultaneous}对$k$-mer特征表示的序列进行归类。
\item  采用$Sensitivity$ 和 $Precision$对效果进行评估。
\end{enumerate}
%our method

在本文中，我们尝试用概率主题模型表示DNA序列的方法来提高MCluster 的性能，并提出了一种新的方法TM-MCluster(Topic Model based Metagenomic reads Clustering 的缩写)。这个方法包含三步：

\begin{enumerate}
\item  用$k$-mer频率向量来表示序列。
 \item  通过LDA~\cite{blei2003}模型将频率向量转化为主题分布向量。
 \item  和MCluster\cite{liao2014new}相似，同样采用SKWIC 算法对这些主题分布向量表示的序列进行归类。
\end{enumerate}


我们用模拟和真实数据对新的方法进行评估，并将它与四个现有的归类方法进行比较，包括MetaCluster 3.0/5.0, AbundanceBin 以及MCluster。
实验结果表明，在长序列的模拟数据集上，TM-MCluster取得了优于MetaCluster 3.0， AbundanceBin以及MCluster 的性能;
在短序列的模拟数据上，TM-MCluster在归类性能上也优于MetaCluster 5.0; 在真实数据集上，TM-MCluster优于已有的4个归类算法。

\section{主题模型及其在生物数据处理中的应用}

\subsection{主题模型及LDA}
%LDA introduction
在机器学习和自然语言处理，主题模型是一类在数据集中发现潜在主题信息的一类统计模型。
主题模型最初是用于文本处理，随后被应用到图像，音频以及音乐处理。最近，有一些研究人员应用主题模型来处理生物数据，
例如从MEDLINE的生物医学文献摘要中挖掘蛋白质相互作用网络~\cite{aso2009predicting,zheng2006identifying}，构建mRNA模型集合~\cite{gerber2007hierarchical}，以及研究元基因组功能群落~\cite{chen2012exploiting}。
为了刻画近义词，计算单词和文档距离，Deerwester\cite{deerwester1990indexing}于1990年提出了LSA(Latent Semantic Analysis)； 为了更好刻画一次多义，采用多项式分布描述词频向量，并将LSA 模型概率化，Hofmann提出pLSA\cite{hofmann1999probabilistic}；2003年，David Blei 和Andrew Ng\cite{blei2003}将pLSA 模型贝叶斯化，采用Dirichlet先验概率，提出经典的(Latent Dirichlet Allocation) LDA 模型。

主题模型最初是用来处理由词袋模型表示的文档，并挖掘潜在主题信息。主题表示隐含的语义主题信息，大量研究结果表明，主题模型比传统的基于关键词模型更能有效的描述文档之间的语义关系。

我们首先介绍LDA模型，然后描述如何应用LDA来抽取元基因组中的潜在主题信息。图\ref{fig:3}中描述了LDA模型。外部碟子表示文档，内部碟子表示一个文档中重复选择的文档和单词。

\begin{figure}[h!]
\centering
    \includegraphics[width=9cm]{./example/LDAModel.eps}
    %\caption{\csentence{The pipeline of the \emph{TM-MCluster} method.}
    \caption{LDA图模型}
            \label{fig:3}
\end{figure}


数学符号定义如下：$D$ 表示文档个数；$N_{d}$ 表示第 $d$ 篇文档的单词个数； $W$ 表示单词表中单词的个数；$T$ 表示主题个数； $\alpha$ ($T$ 维向量) 是每篇文档主题分布的狄利克雷先验的参数； $\beta$ ($W$维向量) 是每个主题的单词分布的狄利克雷先验的参数；$\theta_{d}$ ($T$维向量) 是第$d$ 篇文档的主题分布；$\phi_{j}$ ($W$维向量)是主题$j$的单词分布；
%$z_{ij}$ is the topic for the \emph{j}th word in document \emph{i}; $w_{ij}$ is the generated $j$th word in document $i$.

LDA模型通过下面的过程来生成文档，因此也属于生成模型 \cite{blei2003}:
\begin{enumerate}
\item 对于第$d$ 篇文档，初始化$\alpha$ 为随机值，然后$\theta_{d}\sim$ Dirichlet($\alpha$), $d$$\in$ {1, 2, ..., $D$};
\item 对于第 $t$ 个主题, 初始化 $\beta$ 为随机值, 然后 $\phi_{t}\sim$Dirichlet($\beta$);
\item 对于第 $d$篇文档的$w_{i}$,  $z_{i}\sim$ Multinomial($\theta_{d}$), $w_{i}|z_{i}$ $\sim$ Multinomial($\phi_{z_{i}}$)。
\end{enumerate}


我们采用LDA模型来处理元基因组序列，将序列看作文档，$k$-mer看作单词。对于给定的元基因组序列，可以采用吉布斯采样蒙特拉罗过程来求解LDA 模型的参数
~\cite{griffiths2004finding}。根据后验概率\ref{eq:5}，估计过程需要单独为每个序列每个$k$-mer进行采样：

\begin{equation}\label{eq:5}
P(z_{w_{i}}=j|w{_{i}},\mathbf{w{_{-i}}},\mathbf{z{_{-w{_{i}}}}}) \propto \frac{\beta +n^{w_{i}}_{-i,j}}{W\beta+n^{*}_{-i,j}}\cdot \frac{\alpha +n^{d}_{-i,j}}{T\alpha+n^{d}_{-i,*}}
\end{equation}

$\mathbf{w{_{-i}}}$ 是指除了$w_{i}$以外指派的$k$-mer,
$\mathbf{z{_{-w{_{i}}}}}$ 是指除了$w_{i}$以外为所有$k$-mer 指派的主题
$n^{w_{i}}_{-i,j}$ 是除了$w_{i}$以外，为$k$-mer指定的主题是$j$的个数，
$n^{d}_{-i,j}$ 是指序列$d$中，除了$w_{i}$以外，指定的主题为$j$的$k$-mer总数
$n^{*}_{-i,j}$ 除了$w_{i}$以外，指定的主题为$j$的$k$-mer总数，
$n^{d}_{-i,*}$ 是指序列$d$中除了$w_{i}$以外的$k$-mers总数

通过训练LDA模型，我可以得到每个序列的主题分布。
在我们的模型中，我们设定Dirichlet先验的参数$\alpha$=0.1,  $\beta$=0.01, 这样的参数设定是为了使得主题模型的结果多种多样的。

\subsection{应用}

%\subsubsection{主题模型在蛋白质相互作用网络预测上的应用}
蛋白质序列是指蛋白质的一维结构。对于蛋白质序列的分析可以直接应用在蛋白质远同源性(remote homology)匹配问题上，从而分析未知蛋白质。
想要在主题模型下对蛋白质序列学习，首先要对蛋白质序列进行合适的数据分割定义。合理的数据片段应尽量多的保留信息单元，同时尽量小的切分长度。Pan
\cite{pan2010large}等人采用主题模型来对预测蛋白质之间相互作用。首先，他利用20种常见氨基酸的偶极性和侧链的体积特性，将20种氨基酸分为7类，通过$k$-mer进行特征提取，$k$=3, 通过主题模型可以将$k$-mer 空间变换到主题空间，这样主题分布越相似的蛋白质，越可能存在相互作用。

%\subsubsection{主题模型在蛋白质序列的结构特征表示上的应用}
随着大量扩展的蛋白质结构数据库，基于蛋白质结构的有效查询是一个重要问题。Shivashankar\cite{shivashankar2011multi}等人提出了一种基于主题模型的蛋白质结构特征表示，以此来提高基于蛋白质结构的查询准确性。首先，他将蛋白质序列划分成若干子结构序列，然后根据序列相似度映射到预先定义好的蛋白质片段库，这样蛋白质序列就可以用预先定义好的更加标准的蛋白质片段表示。

具体的，利用LDA模型学习出主题-标准片段分布，每条序列可以利用主题分布来表示。新的相似度计算公式如\ref{eq:6}、\ref{eq:7}所示
\begin{equation}\label{eq:6}
FragSimilarity(A, B)=cos(\theta)=\frac{A\cdot B}{\left \| A \right \|\left \| B \right \|}
\end{equation}
\begin{equation}\label{eq:7}
Similarity = \lambda_1*FragSimilarity + \lambda_2*TopicSimilarity
\end{equation}
此处$FragSimilarity$表示蛋白质片段的余弦相似度，$TopicSimilarty$ 表示蛋白质主题分布的相似度，此处采用KL散度来计算\ref{eq:8}：
\begin{equation}\label{eq:8}
D_{KL}(P\parallel Q)=\prod_{i}P(i)log\frac{P(i)}{Q(i)}
\end{equation}
然后利用权重$\lambda$衡量片段结构和主题向量相似度的权重。


%\subsubsection{主题模型在元基因组序列功能模块识别问题上的应用}
Chen\cite{chen2010probabilistic} 等人采用主题模型来识别同一物种公有的基因特征，分析其生物意义。首先他采用$k$-mer对序列进行特征提取，然后应用主题模型LDA\cite{blei2003}进行特征空间变化，以此来学习基因层次的统计模式。LDA模型统计出基因片段的共现性后，结合基于组成成分(composition-based)和基于同源性(homology-based) 的方法学习出
元基因组序列上的关键功能模块。

%\subsubsection{主题模型在生物医学文本中的挖掘中的应用}
Markus 使用主题模型来模拟PudMed生物文献数据库摘要索引的生成过程\cite{bundschus2008statistical}。如图\ref{fig:4}所示，$D$表示数据集中文档的个数，其中一篇文档$d$
表示为长度为$N_d$的单位向量和长度为$M_d$的MeSH(Medical Subject headings thesaurus)索引概念向量。这样每篇文档的生成过程就分为两步：
首先从单词表选择$w_i$生成文档主体，然后从MeSH中选择关键词$c_i$生成摘要索引部分。

\begin{figure}[h!]
\centering
    \includegraphics[width=9cm]{./example/Topic-Concept.pdf}
    %\caption{\csentence{The pipeline of the \emph{TM-MCluster} method.}
    \caption{主题-概念模型}
            \label{fig:4}
\end{figure}

主题概念不但模拟了文档生成过程，而且模拟了文档被MeSH概念索引的部分。与原始LDA相比，主题概念模型学习出更加丰富的主题，而且提供了重要的额外信息。

\section{深度学习及其在生物数据处理中的应用}
\subsection{深度学习概述}
深度学习是机器学习的一块新的研究领域，它让机器学习更靠近人工智能的目标。与传统机器学习算法不同，深度学习作为表征学习的一种，在学习的过程中，不需要手工设计数据的特征，而是利用多层神经网络的深层结构，由浅层到深层自动学习对目标有利的数据特征表示，另外，深度学习算法学习到的数据特征表示能够获得数据的内部结构，对于不同的学习任务(例如：分类、回归等)，这种表示都可以使用。而机器学习的特征设计大都是基于特定任务的，不同的学习任务，设计出来的特征不同。
因此深度学习是一种新型的基于多层神经网络结构的学习算法，它与传统人工神经网络的训练不同，后向传递(Back Propagation, BP)算法作为传统多层神经网络的经典训练算法，在多层网络的训练过程中结果很不理想\cite{bengio2009learning}。Bengio等人\cite{hinton2006fast, bengio2007greedy} 基于深度信念网(Deep Belief Network, DBN)提出无监督逐层训练算法，为解决深层结构相关优化难的问题带来希望。LeCun等人\cite{lecun1998gradient} 提出的卷积神经网络(Convolutional Neutral Network)是第一个真正多层结构学习算法，它利用空间相对关系减少参数数目以提高BP训练性能。

%\subsection{深度学习的理解}
数据从输入到输出可以用一个流向图(Flow Graph)来表示\cite{bengio2009learning}，流向图的每个节点表示计算的一步和该步计算得到的值。考虑一个计算集允许每个节点和可能的图结构，并且定义了一个函数族。输入节点没有子节点，输出节点没有父节点。这种流向图的一个特别属性是深度(depth): 从输入到输出的最长路径的长度。

传统的前馈神经网络能够被看作拥有等于层数的深度，SVM的深度为2（一个对应于核输出或者特征空间，另外一个对应着输入的线性组合）。
借助深度学习的算法，人类对``抽象概念''的处理有了解决方法。在技术手段方面，有云计算对大数据的并行处理能力。

%\subsection{深度学习的实质}
\iffalse
通过构建具有很多隐层的机器学习模型和海量的训练数据，来学习更有用的特征，从而最终提升分类或预测的准确性。因此，``深度模型''是手段，``特征学习''是目的。与传统的浅层学习不同，深度学习 1)强调了模型结构的深度，即多层隐藏层；2)明确突出了特征学习的重要性，也就是说，通过逐层特征变换，将样本在原空间的特征表示变换到一个新特征空间，从而使分类或预测更加容易。与人工规则构造特征的方法相比，利用大数据来学习特征，更能够刻画数据的丰富内在信息。
\fi
%\subsection{深度学习的动机和原因}
深度学习的动机在于建立、模拟人脑进行分析学习的神经网络，它模仿人脑的机制来解释数据\cite{bengio2009learning}，例如图像，声音和文本。
传统神经网络的训练算法的缺点\cite{bengio2013deep}：
\begin{enumerate}
\item 容易出现过拟合，参数很难调节。
\item 训练速度较慢，当神经网络的层数太多时（大于7层），残差传播到最前面的网络层会变得很小，出现梯度扩散的现象。
\item 收敛到局部最小值。
\item 只能使用有标签数据来训练，但是实际应用中数据大部分是无标签的。
\end{enumerate}

卷积神经网络(Convolutional Neural Network, CNN)作为第一个真正成功训练多层网络结构的学习算法，与DBN不同，它属于判别型模型，卷积结构的使用也是深度学习在语音、图像和自然语言处理中取得成功的关键因素。卷积神经网络由卷积层和次抽样层组成，其隐藏层的单元有一个时间或者空间位置且只与特定窗口的原始输出的值有关系。CNN 作为深度学习框架是基于最小化预处理数据要求而产生的。受早期的时延神经网络影响，CNN 靠共享时域权值降低复杂度。CNN 是利用空间关系减少参数数目以提高一般前向BP 训练的一种拓扑结构，在 CNN中被称做局部感受区域的图像的一小部分作为分层结构的最底层输入。信息通过不同的网络层次进行传递，因此在每一层能够获取对平移、缩放和旋转不变的观测数据的显著特征。

之后，不断有新的深层网络结构的提出，和新的训练技巧的提出，如栈式自动编码机，层叠受限玻尔兹曼机，深度玻尔兹曼机，卷积深度信念网等新型网络结构，以及Dropout，Maxout，稀疏性(Sparsity)，权值衰减(Weight-decaying)，去噪(denoising)等技巧的提出，极大地丰富了深度学习算法及其学习框架，使得深度学习的应用也不再局限于语音和图像方面，在自然语言处理\cite{collobert2011natural}，时间序列数据建模\cite{boulanger2014phone} 方面都取得了很好的成果。

深度学习算法分为有监督学习和无监督学习两种，卷积神经网络属于有监督学习，而深度信念网和自动编码机则属于无监督学习。更严格的讲，对于要解决的实际问题，使用无监督学习算法进行预训练神经网络的权值，然后用来初始化深层神经网络这样网络的性能就会极大提升\cite{erhan2010does}。对于无监督预训练算法，自动编码机在概念上比较简单，但是受限玻尔兹曼机模型对于不同的参数或者不同类型的可视单元和隐藏单元有不同的模型。因此近年来，对于自动编码机的研究偏重于正则化（稀疏、去噪等），而受限玻尔兹曼机不同，它是一种生成模型(Generative Model)，可以抽取样本。

\subsection{自动编码机}
自动编码机属于无监督学习的一种，因为在计算机视觉，语音处理和自然语言处理领域，传统的有监督学习需要人工设计特征，如果设计的特征比较好，那么有监督学习的算法就很有效，但是特征设计是一件费时费力的工作，而且这种特征对于特定问题可以很好地解决，对于其他问题就需要重新设计特征。

自动编码机学习的目标值等于输入值，它学习到的函数是一个恒等函数\cite{alain2012regularized}，它的意义在于学习输入数据的一种表示，这种表示可以重构输入数据，这种表示就是特征。它的结构示意图见图\ref{fig:9}:

\begin{figure}[h!]
    \centering
    \includegraphics[width=9cm]{./example/AutoEncoder.jpg}
    %\caption{\csentence{The pipeline of the \emph{TM-MCluster} method.}
    \caption{自动编码机的结构}
            \label{fig:9}
\end{figure}

设自动编码机的输入为$x \epsilon [0, 1]^{d}$，到隐藏层的第一层映射（也称为编码机）得到的隐含表示 $y \epsilon [0, 1]^{d}$
\begin{equation}\label{eq:16}
y=s(Wx+b)
\end{equation}
从隐含表示$y$映射到输出$z$来重构输入（也称解码机），$z$和$x$的尺寸大小相同。
\begin{equation}\label{eq:17}
z=s(W'y+b')
\end{equation}
这里 并不表示转置，$z$可以看作为$x$的预测。$W'$一般约束为$W$的转置，这样可以减少训练的参数，但是不加这个约束也可以训练自动编码机。

该模型的参数有$W$, $b$, $b'$(如果不加约束项，还包括$W'$),优化目标是平均重构误差最小。重构误差可以用很多评定方法，具体选择与输入数据的分布假设有关\cite{AES}。 一般使用传统的方差测评$L(x, z)=\left \| x-z \right \|^{2}$，或者如果输入数据是二值向量，可以使用交叉熵来测评：
\begin{equation}\label{eq:18}
L_{H}(x,z)=-\sum_{k=1}^{d}[x_klogz_k+(1-x_{k})log(1-z_{k})]
\end{equation}
我们希望得到的中间表示$y$是输入$x$的分布式表示，当隐含单元数目较小时，自动编码机被迫去学习输入数据的压缩表示，$y$一般是$x$的有损压缩，但是如果输入是完全随机的，这个压缩表示将很难学习到，如果输入数据本身具有某种隐藏的特定结构，那么自动编码机就会发现输入数据中的某些相关性，从而将数据输入用低维表示。当隐含单元数目较大时，需要给自动编码机增加一些限制条件来发现输入数据的结构，否则学习到的将是无用的信息，具体地，如果给隐藏神经元加入稀疏性限制，那么自动编码机将会学习到输入数据中的一些有趣的结构。

令人惊讶的是，当隐含单元个数大于输入数据单元时，非线性自动编码机可以学习到更加有用的表示\cite{DA}。 较为合理的解释是，使用随机梯度下降法训练自动编码机，提前终止迭代与对参数加$L2$ 规范项类似。

一个标准的自动编码机的学习算法如算法\ref{alg:ae}所示(一个隐藏层的自动编码机)

\begin{algorithm}
\begin{footnotesize}
\caption{自动编码机学习算法}\label{alg:ae}
\begin{algorithmic}[1]
\REQUIRE ~~\\
$k$-mer特征表示的序列\\
\ENSURE ~~\\
自动编码机学习到的特征表示的序列

\STATE 采用前向传播, 计算$L_{2}$, $L_{3}$层的激活值;
\STATE 计算输出层$L_{3}$的每个单元$i$的误差值:
\begin{equation}\label{eq:19}
        \delta ^{(3)}=-(y-z)\bullet f^{'}(s^{(3)})
\end{equation}
\STATE 对于$l$=2, 1 设
\begin{equation}\label{eq:20}
\delta^{(l)}=((W^{(l)})^{T} \delta ^{(l+1)})\bullet(f'(s^{(l)}))
\end{equation}
\STATE 计算损失函数关于参数的偏导数
\begin{equation}\label{eq:21}
\frac{\partial J(W,b;x,y)}{\partial W^{(l)}}=\delta^{(l+1)}(a^{(l)})^{T}
\end{equation}
\begin{equation}\label{eq:22}
\frac{\partial J(W,b;x,y)}{\partial b^{(l)}}=\delta^{(l+1)}
\end{equation}
\STATE 使用L-BFGS算法求解神经网络的参数
\STATE 更新参数
\end{algorithmic}
\end{footnotesize}
\end{algorithm}



\subsection{限制玻尔兹曼机(RBM)}
%\subsubsection{RBM的结构}

\begin{figure}[h!]
\centering
    \includegraphics[width=11cm]{./example/RBM.eps}
    %\caption{\csentence{The pipeline of the \emph{TM-MCluster} method.}
    \caption{RBM的结构}
            \label{fig:11}
\end{figure}

RBM是一种无向图模型，如图~\ref{fig:11}，它由两层神经单元组成，下面一层称为可视层，上面一层称为隐含层，RBM可视层的每个单元与隐含层的n个单元相连接，和其他可视层单元相互独立，隐含层单元类似。RBM模型可视层单元$v$隐含层单元$h$的联合分布服从Boltzmann分布。RBM的参数有隐含层与可视层连接的权重矩阵$W_{n*m}$，可视层单元的偏移量$b=\{b_{1}，b_{2}...b_{m}\}$，隐含层单元的偏移量$c=\{c_{1},c_{2}...c_{n}\}$，这些参数决定了RBM 网络把一个$m$ 维的样本编码成一个$n$ 维的样本，或者成为提取原始数据的$n$个特征。

RBM是一种对称的结构，因此其训练过程相对简单。对于一个训练样本$x=\{x_{1},x_{2}...x_{m}\}$，隐含层第$i$ 个单元的取值为1的概率为
\begin{equation}\label{eq:23}
P(h_{i}=1 | v)=\sigma (\sum_{j=1}^{m} w_{ij}*v_{j}+c_{i})
\end{equation}

其中$v$的取值即为$x$，为sigmoid函数。生成$n$维样本$y$的过程为：
\begin{enumerate}
\item 利用式\ref{eq:23}计算的概率，也是样本$y$第$i$个分量取值为1的概率。
\item 产生[0, 1]区间内的一个随机数，如果它小于$p(h_{i}=1 | v)$，则$y_{i}=1$，否则$y_{i}=0$。 一般产生一个服从均匀分布的随机数。
\end{enumerate}

因为RBM的结构对称，已知隐含层的样本$y$，生成可视层的样本$x$的过程原理相同。上述过程可以看作编码过程，下面就是解码过程：

\begin{enumerate}
\item 利用式\ref{eq:24}计算$p(v_{j}=1|h)$的概率。
\begin{equation}\label{eq:24}
P(v_{j}=1 | h)=\sigma (\sum_{i=1}^{n} w_{ij}*h_{i}+b_{j})
\end{equation}%\subsubsection{RBM的用途}
\item 产生区间内的一个随机数，如果它小于$p(v_{j}=1|h)$的，则$x_{j}=1$，否则$x_{j}=0$。
\end{enumerate}

可视层第$j$个单元的取值为1的概率为：
RBM主要有两个用途:

第一种是对数据进行编码，作为特征提取，将得到的编码交给监督学习方法进行分类和回归，也可以用作降维的方法使用(即隐含层单元数量$n<m$)。

第二种是将得到的权重矩阵和偏移量作为深层网络结构的训练初始化参数。因为深层网络结构直接使用BP训练，如果初始值选取不当，往往会陷入局部最优，实际应用结果表明，直接把RBM训练得到的权重和偏移量作为BP神经网络的初始值，得到的效果会非常好。
%\subsubsection{RBM能量模型}
因为玻尔兹曼网络是一种随机网络，描述一个随机网络，主要有两种方法\cite{bengio2009learning}：

第一种，概率分布函数。由于网络节点的取值状态是随机的，从贝叶斯网的观点来看，要描述整个网络需要三种概率分布：联合概率分布、边缘概率分布和条件概率分布。而从贝叶斯网的观点来看，RBM 可以看作是一个双向的有向图，即从输入层单元可以计算隐含层单元的某一种状态的概率，反之亦然。

第二种，能量函数。随机神经网络是根植于统计力学的。受统计力学中能量泛函的启发，引入了能量函数。能量函数是描述整个系统状态的一种测度，系统越有序或者概率分布越集中，系统的能量越小，反之，系统越无序或者概率分布越趋于均匀分布，系统的能量越大。能量函数的最小值，对应于系统的最稳定状态。

能量模型在马尔科夫随机场（MRF）中主要有两个作用：

一、全局解的度量（目标函数）；

二、能量最小时的解（各种变量对应的配置）为目标解。

RBM是一种概率图模型，引入概率是为了采样（sampling）方便，因为在对比散度(contrastive divergence, CD)算法中采样部分是模拟求解梯度的关键。

RBM的能量函数的定义为：
\begin{equation}\label{eq:25}
E(v, h)=-\sum_{i=1}^{n}\sum_{j=1}^{m}w_{ij}h_{i}v_{j}-\sum_{j=1}^{m}b_{j}v_{j}-\sum_{i=1}^{n}c_{i}h_{i}
\end{equation}
式\ref{eq:25}表明RBM的能量由可视单元和隐含单元之间的连接的权重和偏移量决定的。
假设基于这个能量模型求解RBM，需要将所有可视单元和隐含单元的可能取值的能量累加作为目标函数，然而这个计算量是随着隐含单元的数量和状态数目的增长呈指数增加。为了简化这一繁杂的计算过程，引入概率，而的定义是基于能量函数：
\begin{equation}\label{eq:26}
p(v,h|\theta)=\frac{e^{-E(v,h|\theta)}}{Z(\theta)}
\end{equation}
\begin{equation}\label{eq:27}
Z(\theta)=\sum_{v,h}e^{-E(v,h|\theta)}
\end{equation}

这里为了简化参数表示用表示权重矩阵，偏移量b和c，为归一化因子。

%\subsubsection{Gibbs采样}
Gibbs采样是一种基于马尔科夫蒙塔卡罗策略的采样方法。具体来说，对于一个d维的随机向量$x=(x_{1},x_{2}...x_{d})$，假设联合概率分布$p(x)$未知，对于第$i$个分量$x_{i}$，已知条件分布$p(x_{i}|x_{1}....x_{d})$，则可对第i个分量进行采样，随着采样次数的增加，随机变量$x(n)$ 的分布会收敛到$x$ 的联合概率分布$p(x)$。
基于RBM的特殊结构，Gibbs采样非常快速，对于$k$步Gibbs采样，对于初始训练样本$v_{0}$：
\begin{equation}\label{eq:28}
\begin{split}
    h_{0}\sim P(h|v_{0})\\
    v_{1}\sim P(v|h_{0})\\
    h_{1}\sim P(h|v_{1})\\
    v_{2}\sim P(v|h_{1})\\
    h_{k}\sim P(h|v_{k})\\
    v_{k+1}\sim P(v|h_{k})
\end{split}
\end{equation}
式\ref{eq:28}进行了$k$步采样，一般$k$要很大，比较费时，Hinton提出了一个简化的Gibbs采样方法[17]，可以以较少的步数收敛到样本的联合概率分布。
%\subsubsection{基于对比散度的快速学习算法}
对比散度是Hinton在2005年提出的一种训练RBM的方法\cite{carreira2005contrastive}，其主要思想是使用训练样本初始化，然后进行$k$次Gibbs 抽样，就可以得到较好的近似。而实际应用时$k$取值并不需要太大，本文$k$取2。
其算法伪代码如\ref{alg:rbm} 所示。

\begin{algorithm}
\begin{footnotesize}
\caption{k步对比散度}\label{alg:rbm}
\begin{algorithmic}[1]
\REQUIRE ~~\\
RBM($V_{1},...V_{m},H_{1},...H_{n}$), 训练样本$S$\\
\ENSURE ~~\\
近似梯度

\STATE 初始化 $\Delta w_{ij}=\Delta b_{j} = \Delta c_{i} = 0, i=1,...n, j=1...m$;
\STATE
\FOR {each $v \in S$}
    \STATE $v_{0}\leftarrow v$;
    \FOR {$t=0...k-1$}
        \STATE Gibbs采样得到$h^{t}$;
        \STATE Gibbs采样得到$v^{t+1}$;
    \ENDFOR
\ENDFOR

\FOR {$i=1...n$}
    \FOR{$j=1...m$}
        \STATE $\Delta w_{ij} \leftarrow \Delta w_{ij}+p(H_{i}=1|v^{(0)})\cdot v_{j}^{(0)}-p(H_{i}=1|v^{k})\cdot v_{j}^{(k)}$ \\
        \STATE $\Delta b_{j} \leftarrow \Delta b_{j}+v_{j}^{(0)}-v_{j}^{(k)}$ \\
        \STATE $\Delta c_{i} \leftarrow \Delta c_{j} + p(H_{i}=1| v^{(0)})-p(H_{i}=1|v^{(k)})$
    \ENDFOR
\ENDFOR
\end{algorithmic}
\end{footnotesize}
\end{algorithm}
\subsection{应用}

%\subsubsection{深度学习在蛋白质结构预测上的应用}
蛋白质结构预测是通过氨基酸序列来预测蛋白质的三维结构，换句话说，是通过蛋白质的一级结构来预测他的二级，三级和四级以及折叠结构。蛋白质结构预测和蛋白质设计的反问题从根本上说是不同的。蛋白质结构预测是生物信息和理论化学领域非常重要的问题，尤其在医学(例如药物设计), 和生物科技(例如新的酶的设计)至关重要。每两年，当前方法的性能在CASP实验中就会进行评估一次。

从头开始的蛋白质二级结构预测被用来进行三级结构预测，由于蛋白质组学的快速发展，预测的需求急剧增加。然而，最近的方法已经停滞在80\%的准确率上，
而且不能打破这个上限了。Matt Spencer等人\cite{spencer2014deep}, 提出一种深度学习方法来预测蛋白质二级结构。该方法采用位置打分矩阵(PSSM), 以及氨基酸残基(RES)。

他们的深度网络以及取得了80.7\%的准确性，也成为了目前最好的方法。在他们的网络中，他们采用逐层非监督预训练方法来学习DBN。和其他方法例如PSSpred，SSpro，PSIPRED，RaptorX在数据集CASP9和CASP10的结果相比，
他们的方法和PSSpred是得分最高的工具。

%\subsubsection{深度学习在可变剪切上的应用}
可变剪切在生物复合物和它在人类疾病中的反向调控扮演着至关重要的作用。此处，我们描述了剪切编码的装配，它采用RNA几百个特征的组合来预测器官相关
变化。

以前对于剪切编码的研究是基于贝叶斯神经网路(BNN)的，其中一个优势就是BNN可以通过集成模型来防止过拟合。然而，当处理大数据集时，MCMC方法可能会非常慢。Leung\cite{leung2014deep}提出了一个DNN模型，该模型可以同时预测单个器官中的剪切模式和不同器官之间的模式差异。这个DNN结构有三层隐含层，输出层由三个softmax 分类构成。结构显示，DNN性能优于BNN，因为深层结构可以处理数据中的复杂关系。

%\subsubsection{深度学习在疾病诊断上的应用}
研究人员已经采用机器学习方法来进行疾病诊断并且取得了非常好的结果。特别地，贝叶斯网络解决这个问题具有天然优势，其他得方法例如SVM和神经网络也同样效果不错。然而当面临一些不太常见的疾病例如癌症仍然有很多挑战。由于这些疾病不太常见，训练样本集较小，而且他们的特征空间往往是高维的。这些问题难以解决，并且容易出现过拟合。用来训练的特征往往是手动设计，因此特征是与任务有关的。最近，Fakoor\cite{fakoor2013using}采用了非监督的特征学习，以及深度学习方法来处理癌症诊断，并提出了一个较一般性的癌症分类器。他采用了自动编码机和稀疏自动编码机进行特征学习。首先，他们采用PCA进行特征降维来保留信息内容。
其次，他们运用自动编码机来学习特征的表示。最终他们采用学习的特征来训练分类器。

\section{小结}
在本章中，我们介绍了元基因组归类问题的相关工作，包括目前已有的两类算法：基于序列相似度以及基于组成成分两类归类算法。随后介绍了机器学习中的主题模型，以LDA作为代表模型，简单介绍了LDA 的来龙去脉，LDA图模型，采用Gibbs Sampling方法进行求解等，以及目前主题模型在生物数据中是如何来解决问题的，包括蛋白质相互作用网络预测，蛋白质序列结构特征表示，元基因组序列功能模块识别，以及生物医学文本中的挖掘。此外我们还介绍了目前非常流行的深度学习概念，自动编码机和RBM，以及深度学习在蛋白质结构预测，可变剪切和疾病诊断上的应用。

