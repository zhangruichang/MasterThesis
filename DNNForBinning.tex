\chapter{基于深度学习的元基因组归类算法}
\rhead{基于深度学习的元基因组归类算法}

基于深度学习的归类算法也同样包含三步：1) 用$k$-mer频率向量来表示序列 ~2) 用自动编码机(或RBM)对$k$-mer特征进行特征学习 ~3) 采用SKWIC
算法对向量化的序列进行归类。图~\ref{fig:10}中显示了工作流程。
\begin{figure}[h!]
\centering
    \includegraphics[width=17cm]{./example/AEModulePipeline.eps}
    %\caption{\csentence{The pipeline of the \emph{TM-MCluster} method.}
    \caption{基于深度学习的元基因组归类算法的工作流程}
            \label{fig:10}
\end{figure}

\section{基于自动编码机的特征学习}
自动编码机学习的目标值等于输入值，它学习到的函数是一个恒等函数\cite{alain2012regularized}，它的意义在于学习输入数据的一种表示，这种表示可以重构输入数据，这种表示就是特征。它的结构示意图见图\ref{fig:9}:

\begin{figure}[h!]
    \centering
    \includegraphics[width=9cm]{./example/AutoEncoder.jpg}
    %\caption{\csentence{The pipeline of the \emph{TM-MCluster} method.}
    \caption{自动编码机的结构}
            \label{fig:9}
\end{figure}

设自动编码机的输入为$x \epsilon [0, 1]^{d}$，到隐藏层的第一层映射（也称为编码机）得到的隐含表示 $y \epsilon [0, 1]^{d}$
\begin{equation}\label{eq:16}
y=s(Wx+b)
\end{equation}
从隐含表示$y$映射到输出$z$来重构输入（也称解码机），$z$和$x$的尺寸大小相同。
\begin{equation}\label{eq:17}
z=s(W'y+b')
\end{equation}
这里 并不表示转置，$z$可以看作为$x$的预测。$W'$一般约束为$W$的转置，这样可以减少训练的参数，但是不加这个约束也可以训练自动编码机。

该模型的参数有$W$, $b$, $b'$(如果不加约束项，还包括$W'$),优化目标是平均重构误差最小。重构误差可以用很多评定方法，具体选择与输入数据的分布假设有关\cite{AES}。 一般使用传统的方差测评$L(x, z)=\left \| x-z \right \|^{2}$，或者如果输入数据是二值向量，可以使用交叉熵来测评：
\begin{equation}\label{eq:18}
L_{H}(x,z)=-\sum_{k=1}^{d}[x_klogz_k+(1-x_{k})log(1-z_{k})]
\end{equation}
我们希望得到的中间表示$y$是输入$x$的分布式表示，当隐含单元数目较小时，自动编码机被迫去学习输入数据的压缩表示，$y$一般是$x$的有损压缩，但是如果输入是完全随机的，这个压缩表示将很难学习到，如果输入数据本身具有某种隐藏的特定结构，那么自动编码机就会发现输入数据中的某些相关性，从而将数据输入用低维表示。当隐含单元数目较大时，需要给自动编码机增加一些限制条件来发现输入数据的结构，否则学习到的将是无用的信息，具体地，如果给隐藏神经元加入稀疏性限制，那么自动编码机将会学习到输入数据中的一些有趣的结构。

令人惊讶的是，当隐含单元个数大于输入数据单元时，非线性自动编码机可以学习到更加有用的表示\cite{DA}。 较为合理的解释是，使用随机梯度下降法训练自动编码机，提前终止迭代与对参数加$L2$ 规范项类似。
我们采用了栈式自动编码机进行特征学习。栈式自动编码机是指对将多个自动编码机进行层叠，进行特征学习。当$k$=4时，我们对于采用不同个数的自动编码机，不同的隐单元个数的归类性能进行比较分析，分别采用一层神经网络，隐单元各位分别为512和1024，两层神经网络，隐单元1024、512，三层神经网络，隐单元个数1024、1024、512的栈式自动编码机进行特征学习。


批处理的样本个数为1000，最大迭代次数为500。最终学习的特征个数为512个。我们采用了LBFGS算法\cite{ngiam2011optimization}来求解自动编码机中的参数。

\section{基于RBM的特征学习}
RBM是一种无向图模型，如图~\ref{fig:11}，它由两层神经单元组成，下面一层称为可视层，上面一层称为隐含层，RBM可视层的每个单元与隐含层的n个单元相连接，和其他可视层单元相互独立，隐含层单元类似。RBM模型可视层单元$v$隐含层单元$h$的联合分布服从Boltzmann分布。RBM的参数有隐含层与可视层连接的权重矩阵$W_{n*m}$，可视层单元的偏移量$b=\{b_{1}，b_{2}...b_{m}\}$，隐含层单元的偏移量$c=\{c_{1},c_{2}...c_{n}\}$，这些参数决定了RBM 网络把一个$m$ 维的样本编码成一个$n$ 维的样本，或者成为提取原始数据的$n$个特征。

RBM是一种对称的结构，因此其训练过程相对简单。对于一个训练样本$x=\{x_{1},x_{2}...x_{m}\}$，隐含层第$i$ 个单元的取值为1的概率为
\begin{equation}\label{eq:23}
P(h_{i}=1 | v)=\sigma (\sum_{j=1}^{m} w_{ij}*v_{j}+c_{i})
\end{equation}

其中$v$的取值即为$x$，为sigmoid函数。生成$n$维样本$y$的过程为：
\begin{enumerate}
\item 利用式\ref{eq:23}计算的概率，也是样本$y$第$i$个分量取值为1的概率。
\item 产生[0, 1]区间内的一个随机数，如果它小于$p(h_{i}=1 | v)$，则$y_{i}=1$，否则$y_{i}=0$。 一般产生一个服从均匀分布的随机数。
\end{enumerate}

因为RBM的结构对称，已知隐含层的样本$y$，生成可视层的样本$x$的过程原理相同。上述过程可以看作编码过程，下面就是解码过程：

\begin{enumerate}
\item 利用式\ref{eq:24}计算$p(v_{j}=1|h)$的概率。
\begin{equation}\label{eq:24}
P(v_{j}=1 | h)=\sigma (\sum_{i=1}^{n} w_{ij}*h_{i}+b_{j})
\end{equation}%\subsubsection{RBM的用途}
\item 产生区间内的一个随机数，如果它小于$p(v_{j}=1|h)$的，则$x_{j}=1$，否则$x_{j}=0$。
\end{enumerate}

可视层第$j$个单元的取值为1的概率为：
RBM主要有两个用途:

第一种是对数据进行编码，作为特征提取，将得到的编码交给监督学习方法进行分类和回归，也可以用作降维的方法使用(即隐含层单元数量$n<m$)。

第二种是将得到的权重矩阵和偏移量作为深层网络结构的训练初始化参数。因为深层网络结构直接使用BP训练，如果初始值选取不当，往往会陷入局部最优，实际应用结果表明，直接把RBM训练得到的权重和偏移量作为BP神经网络的初始值，得到的效果会非常好。
%\subsubsection{RBM能量模型}
因为玻尔兹曼网络是一种随机网络，描述一个随机网络，主要有两种方法\cite{bengio2009learning}：

第一种，概率分布函数。由于网络节点的取值状态是随机的，从贝叶斯网的观点来看，要描述整个网络需要三种概率分布：联合概率分布、边缘概率分布和条件概率分布。而从贝叶斯网的观点来看，RBM 可以看作是一个双向的有向图，即从输入层单元可以计算隐含层单元的某一种状态的概率，反之亦然。

第二种，能量函数。随机神经网络是根植于统计力学的。受统计力学中能量泛函的启发，引入了能量函数。能量函数是描述整个系统状态的一种测度，系统越有序或者概率分布越集中，系统的能量越小，反之，系统越无序或者概率分布越趋于均匀分布，系统的能量越大。能量函数的最小值，对应于系统的最稳定状态。

能量模型在马尔科夫随机场（MRF）中主要有两个作用：

一、全局解的度量（目标函数）；

二、能量最小时的解（各种变量对应的配置）为目标解。


RBM是一种无向图模型。它由两层神经单元组成，下面一层称为可视层，上面一层称为隐含层。
我们采用Gibbs采样的方法求解RBM中的参数。对于输入的$k$-mer特征，我们输出学习后的特征。


\section{模拟数据集}
由于深度学习对于较大的数据集效果较好，因此我们采用500k, 1000k和3000k的序列进行特征学习。具体的，500k的数据集S5、S6、S7、S8、S9 和S10，1000k的数据集A、B，3000k的数据集为C、D、E、F和G。具体的数据说明在上一章数据集已经进行了说明。

评价标准依然采用$Precision$、$Sensitivity$以及$F1$作为归类算法的评价标准。

\section{实验结果}

\subsection{$k$-mer的影响}

当$k$增大时，特征会变得稀疏，然而深度学习是可以处理这种稀疏性的。
我们固定栈式自动编码机的结构，经过之前的实验，我们选择三层自动编码机，神经单元个数分别为1024,1024,512。
当$k$从2变化到6的时候，我们分析栈式自动编码机的归类性能。实验结果如\ref{fig:10}所示：

\begin{figure}[h!]
\centering
    \includegraphics[width=17cm]{./example/KmerAnalysis1.eps}
    %\caption{\csentence{The pipeline of the \emph{TM-MCluster} method.}
    \caption{$k$-mer对于自动编码机特征学习的影响}
            \label{fig:10}
\end{figure}
可以看出$k$较小时，特征个数较少，不足以刻画序列的特征，自动编码机也不能更好的抽取更加好的特征。
当$k$较大时，特征个数过多，自动编码机学习会产生过拟合，导致归类效果下降。
当$k$=5时，$Precision$、$Sensitivity$和$F1$值分别为0.7748， 0.7659和0.7703， 都优于$k$=2,3,4以及$k$=6时，实验结果也说明了这一点。



\iffalse

\begin{table}[htbp]
\centering
    \caption{$k$=4和5时，自动编码机在500k序列的模拟数据上的归类结果。加粗数值表示某个数据集上的最好结果。}
    \begin{tabular}{|c|c|c|c|c|c|c|}
    \hline
    \multirow{2}{*}{\textbf{Dataset}} & \multicolumn{3}{|c|}{\textbf{$k$=4}}&\multicolumn{3}{|c|}{\textbf{$k$=5}}\\
    \cline{2-7}
    &Pr&Se&F1&Pr&Se&F1\\
    \hline
    S6	& .9889&	.9889&	.9889&	\textbf{.9913}&	\textbf{.9913}&	\textbf{.9913}\\
    \hline
    S7	& 	\textbf{.9022}&	\textbf{.9022}&	\textbf{.9022}&	.7325&	.7646&	.7482\\
    \hline
    S8	& 	.8450&.8450	&.8450&	\textbf{.8632}&	\textbf{.8632}&	\textbf{.8632}\\
    \hline
    S9	& 	\textbf{.8292}&	\textbf{.8292}&	\textbf{.8292}&	.8244&	.7219&	.7698	\\
    \hline
    S10	& 	.7216&	.715&	.7183&	\textbf{.7748}&	\textbf{.7659}&	\textbf{.7703}	\\
    \hline
    \end{tabular}
    \label{tab:5}
\end{table}

通过查看$k$=4和$k$=5时，500k的5个数据集S6-S10在自动编码机上的性能比较，可以看出在S6、S8和S10上$k$=5时$F1$值更高。
S7和S9上$k$=4时$F1$值更高。

\fi

\subsection{自动编码机个数及神经网络单元个数的影响}
我们采用多个自动编码机进行特征的多次编码和学习，这样可以学习到更好的特征。
我们分别采用1个、2个和3个自动编码机的不同神经单元结构进行实验分析。

\begin{table}[htbp]
\centering
    \caption{自动编码机不同层数的归类效果。加粗数值表示某个数据集上的最好结果。}
    \begin{tabular}{|c|c|c|c|}
    \hline
    \multirow{2}{*}{\textbf{神经网络个数及每层单元个数}} & \multicolumn{3}{|c|}{\textbf{AutoEncoder}}\\
    \cline{2-4}
    &Pr&Se&F1\\

  %  \hline
  %  D1&\textbf{.9989}&.9628	&.9805&	.9877&	.9877	&.9877	&.9882	&\textbf{.9882}	&\textbf{.9882}\\
    \hline
    [512]&		.7044&	.7083&	.7064	\\
    \hline
    [1024]&		.4478&	.4092&	.4277\\
    \hline
    [1024 512]&.7158	&.7114	&.7136\\
    \hline
    [1024 1024 512]&\textbf{.7224}	&\textbf{.7169}	&\textbf{.7196}\\
    \hline
    \end{tabular}
    \label{tab:9}
\end{table}

通过实验分析，采用三个自动编码机，层数分别为1024、1024和512时，$Precision$ $Sensitivity$和$F1$值最高，分别为0.7224、0.7169和0.7196。
当栈式自动编码机个数较少时，不能够学习到更好的特征，因此采用一个和两个自动编码机的$Precision$ $Sensitivity$和$F1$值都相对低于采用三个自动编码机的结果。

采用一个自动编码机时，输出特征数为512的归类效果明显优于特征数为1024个的，因此对于两层和三层的自动编码机，我们最后输出的特征个数为512 个。



\subsection{模拟数据集实验结果}
多层自动编码机可以学习到更好的特征，我们选择3个自动编码机进行特征学习，采用4-mer, 输入特征为256， 输出的特征数为512，也即采用了3 个自动编码机(SAE) 进行特征学习，层数分别为1024、1024和512个。RBM采用4-mer，输出特征是1000，执行300轮迭代，稀疏项系数为0.01， 惩罚项系数是0.002。
然后同样采用SKWIC算法对学习到的特征进行归类，并分别查看自动编码机和RBM在长序列和短序列数据集上的实验结果。

长序列数据集上的实验结果如表\ref{tab:9}所示。
\iffalse
                      Precision Sensitivity F1
500k 2class 1-1        0.988922 0.988922 0.988922
500k 3class 1-1-1     0.902198 0.902198 0.902198
500k 3class 1-3-9     0.84502 0.84502 0.84502
500k 5class           0.829206 0.829206 0.829206
500k 10class          0.721574 0.714996 0.71827

1M 20species          0.404149 0.233673 0.296129
1M 50species           0.21230 0.31289  0.252961

3000k 2class            0.819216 0.819216 0.819216
3000k 3class            0.619232 0.619232 0.619232
3000k 3class-abundance  0.980533 0.980533 0.980533
3000k 5class            0.7784 0.977165 0.86653
3000k 10class           0.302841 0.793249 0.438337
\fi
\begin{table}[htbp]
\centering
    \caption{自动编码机和RBM在5个长序列数据集上的归类效果。加粗数值表示某个数据集上的最好结果。}
    \begin{tabular}{|c|c|c|c|c|c|c|}
    \hline
    \multirow{2}{*}{\textbf{Dataset}} & \multicolumn{3}{|c|}{\textbf{AutoEncoder}}  & \multicolumn{3}{|c|}{\textbf{RBM}} \\
    \cline{2-7}
    &Pr&Se&F1 &Pr&Se&F1 \\

  %  \hline
  %  D1&\textbf{.9989}&.9628	&.9805&	.9877&	.9877	&.9877	&.9882	&\textbf{.9882}	&\textbf{.9882}\\
    \hline
    S6&		.9889&	.9889&	.9889	&\textbf{.9913}&	\textbf{.9913}&	\textbf{.9913}\\
    \hline
    S7&		\textbf{.9022}&	\textbf{.9022}&	\textbf{.9022}  &.7325&	.7646&	.7482\\
    \hline
    S8&.8450	&.8450	&.8450 &\textbf{.8632}&\textbf{.8632}	&\textbf{.8632}\\
    \hline
    S9		&\textbf{.8292}&	\textbf{.8292}&	\textbf{.8292} &.8244	&.7219	&.7698\\
    \hline
    S10	&	.7216&	.7150&	.7183 & \textbf{.7764}&	\textbf{.7659}&	\textbf{.7703}\\
    \hline
    \end{tabular}
    \label{tab:9}
\end{table}
%通过实验结果可以知道，自动编码机归类效果在12个数据集上都优于RBM。
S6-S10是序列长度为1000bp，序列个数为500k，物种数分别为2,3,3,5,10的长序列数据集。
我们将AutoEncoder和RBM的参数都调节到最佳，进行性能比较。可以发现S6、S8、S10数据集上，RBM的性能更好，这三个数据集相对物种丰度比分布比较均匀，这也说明RBM更适合在物种丰度比分布均匀的数据集上进行特征学习，而AutoEncoder则更适合在物种丰度比分布不均匀的数据集上进行特征学习，这个自动编码机的模型也是有关联的。特别的，在10个物种的数据集上，RBM取得的$Precision$、$Sensitivity$和$F1$值分别是0.7764, 0.7659和0.7703, 性能非常出色。


短序列数据集上的实验结果如表\ref{tab:10}所示。

\begin{table}[htbp]
\centering
    \caption{自动编码机和RBM在短序列数据集上的归类效果。加粗数值表示某个数据集上的最好结果。}
    \begin{tabular}{|c|c|c|c|c|c|c|}
    \hline
    \multirow{2}{*}{\textbf{Dataset}} & \multicolumn{3}{|c|}{\textbf{AutoEncoder}}  & \multicolumn{3}{|c|}{\textbf{RBM}} \\
    \cline{2-7}
    &Pr&Se&F1 &Pr&Se&F1 \\

  %  \hline
  %  D1&\textbf{.9989}&.9628	&.9805&	.9877&	.9877	&.9877	&.9882	&\textbf{.9882}	&\textbf{.9882}\\
    \hline
    A		&\textbf{.4041}&	.2337&	.2961 & .3748&	\textbf{.2659}&	\textbf{.3111}\\
    \hline
    B		&.2123&	\textbf{.3129}&	.2530 & \textbf{.2777}&	.2659&	\textbf{.2703}\\
    \hline
    C	&.8192&	\textbf{.8192}&	\textbf{.8192} & \textbf{.8759}&	.7643&	.8167\\
    \hline
    D	&	.6192&	.6192&	.6192 & \textbf{.6729}&	\textbf{.6659}&	\textbf{.6703}\\
    \hline
    E		&	\textbf{.9805}&	\textbf{.9805}&	\textbf{.9805} & .8748&	.7623&	.8167\\
    \hline
    F&	.7784&	\textbf{.9772}&	\textbf{.8665} & \textbf{.7976}&	.7616&	.7801\\
    \hline
    G	&	.3028&	\textbf{.7932}&	.4383 & \textbf{.3764}&	.7689&	\textbf{.5033}\\
    \hline
    \end{tabular}
    \label{tab:10}
\end{table}
%通过实验结果可以知道，自动编码机归类效果在12个数据集上都优于RBM。


通过实验分析，C、E和F数据集上，自动编码机学习的特征的归类效果要优化RBM。当序列个数较少时，RBM在多数数据集上学习的特征会更加好，归类效果优于自动编码机，因为序列个数较少时，进行多次的特征提取并不会更好的刻画序列的特征，两层神经网络的RBM可以得到更好的归类效果。当序列个数增加时，采用多层的自动编码机学习的特征能够更加刻画序列的特征，对于3000k序列的数据集C、D、E、F和G，自动编码机在C、E 和F上可以得到更好的归类效果，$F1$值分别为
0.8192、0.9805和0.8665。



\section{小结}
本章中，我们提出了基于深度学习的元基因组归类算法，采用自动编码机和RBM进行特征学习，
然后采用SKWIC算法进行归类。我们分析了$k$-mer对于自动编码机的归类效果的影响，以及不同的栈式自动编码机结构对于归类效果的影响，同时还对自动编码机和RBM的归类效果进行对比分析。

