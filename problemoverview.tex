\chapter{主题模型简介}
\rhead{问题定义与系统概述}
为了刻画近义词，计算word和doc距离，Deerwester于1990年提出了LSA(Latent Semantic Analysis)，为了更好刻画一次多义，采用多项式分布描述
词频向量，并将LSA模型概率化，提出pLSA。2003年,David Belin和Andrew Ng将pLSA模型贝叶斯化，采用Dirichlet先验概率，提出经典的(Latent Dirichlet Allocation) LDA 模型。

主题模型最初是用来处理由词袋模型表示的文档，并发掘潜在主题信息。主题表示隐含的语义主题信息，大量研究结果表明，主题模型比传统的基于关键词模型更能有效的描述文档之间的语义关系。

本文，我们将首先介绍LDA模型，然后描述如何运用LDA来抽取元基因组中的潜在主题信息。图1中描述了LDA模型。外部碟子表示文档，内部碟子表示一个文档中重复选择的文档和单词。

一些符号表示如下：D表示文档个数，Nd表示第d篇文档的单词个数，W表示词典里单词总数，T表示主题的个数，alpha是每个文档主题分布Dirichlet先验的参数，
beta是每个主题单词分布的Dirichlet先验的参数，thetad是文档d的主题分布，phij是主题j的单词分布。

LDA试图通过下面的过程来生成文档：
1. 对于第d篇文档，用随机值初始化alpha，然后

通过训练LDA模型，我可以得到每个序列的主题分布。图2表示LDA在元基因组序列上的运用，左层图表示DNA序列，中间层表示主题，右层图表示k-mer，
我们用每个序列的主题分布来表示序列。由于主题的数量通常小于k-mer的数量，这个过程等价于降维。此处，主题个数是一个可调节参数。在我们实验研究中，我们对于模拟数据和真实数据分别设置主题数为20和100.



Topic models were originally proposed to discover latent topics from a set of documents that are represented by the bag-of-word model. Latent topics indicate implicit semantic topics in documents, which have been shown to be more effective in describing the semantic relationships among documents than traditional keyword based models. In metagenomics, reads can be seen as documents and $k$-mers as keywords in documents. Reads from the same species should share more similar latent ``topics'' than reads from different species. So ``topics'' may be more effective than $k$-mers in describing metagenomic reads as far as the binning problem is considered. Here, we use the Latent Dirichlet Allocation (LDA) --- a popular topic model from machine learning area cite{blei2003}. In what follows, we first introduce the LDA model, and then describe how to employ the LDA to extract latent topics from metagenomic reads.
Fig.~\ref{fig:2} illustrates the LDA model. The outer plate represents documents, while the inner plate represents the repeated choice of topics and words within a document.



\begin{figure}[h!]
\centering
    \includegraphics[width=9cm]{./example/LDAModel.eps}
    %\caption{\csentence{The pipeline of the \emph{TM-MCluster} method.}
    \caption{The LDA model.}
            \label{fig:1}
\end{figure}

\begin{figure}[h!]
\centering
    \includegraphics[width=7cm]{./example/LDARepresentation.eps}
    %\caption{\csentence{The pipeline of the \emph{TM-MCluster} method.}
    \caption{Applying the LDA model to metagenomic reads.}
            \label{fig:2}
\end{figure}

数学符号定义如下: \emph{D} 表示文档个数; $N_{d}$ 表示第 \emph{d} 篇文档的单词个数; \emph{W} 表示单词表中单词的个数; \emph{T} 表示主题个数; $\alpha$ ($T$ 维向量) 是每篇文档主题分布的狄利克雷先验的参数; $\beta$ (\emph{W}维向量) 是每个主题的单词分布的狄利克雷先验的参数; $\theta_{d}$ ($T$维向量) 是第\emph{d}篇文档的主题分布; $\phi_{j}$ (\emph{W}维向量)是主题\emph{j}的单词分布;
%$z_{ij}$ is the topic for the \emph{j}th word in document \emph{i}; $w_{ij}$ is the generated $j$th word in document $i$.

LDA模型通过下面的过程来生成文档，因此也属于生成模型 cite{blei2003}:
\begin{enumerate}
\item 对于第\emph{d} 篇文档, 初始化$\alpha$ 为随机值, 然后$\theta_{d}\sim$ Dirichlet($\alpha$), \emph{d}$\in$ {1, 2, ..., \emph{D}};
\item 对于第 \emph{t} 个主题, 初始化 $\beta$ 为随机值, 然后 $\phi_{t}\sim$Dirichlet($\beta$);
\item 对于第 \emph{d}篇文档的$w_{i}$,  $z_{i}\sim$ Multinomial($\theta_{d}$), $w_{i}|z_{i}$ $\sim$Multinomial($\phi_{z_{i}}$).
\end{enumerate}

我们采样LDA模型来处理元基因组序列，序列看成文档，k-mer看成单词。对于给定的元基因组序列，LDA模型的估计可以采用吉布斯采样蒙特拉罗过程
~\cite{griffiths2004finding}。根据后验概率， 估计过程需要单独为每个序列每个k-mer进行采样：


\begin{equation}\label{eq0}
P(z_{w_{i}}=j|w{_{i}},\mathbf{w{_{-i}}},\mathbf{z{_{-w{_{i}}}}}) \propto \frac{\beta +n^{w_{i}}_{-i,j}}{W\beta+n^{*}_{-i,j}}\cdot \frac{\alpha +n^{d}_{-i,j}}{T\alpha+n^{d}_{-i,*}}
\end{equation}

$\mathbf{w{_{-i}}}$ 是指除了$w_{i}$以外指派的$k$-mer, 
$\mathbf{z{_{-w{_{i}}}}}$ 是指除了$w_{i}$以外为所有$k$-mer 指派的topic 
$n^{w_{i}}_{-i,j}$ 是除了$w_{i}$以外，为k-mer指定的topic是j的个数，
$n^{d}_{-i,j}$ 是指序列d中，除了$w_{i}$以外，指定topic为j的k-mer总数
$n^{*}_{-i,j}$ 除了$w_{i}$以外，指定的topic为j的k-mer总数，
$n^{d}_{-i,*}$ 是指序列d中除了$w_{i}$以外的$k$-mers总数


where $\mathbf{w{_{-i}}}$ is the current assignment $k$-mers except for $w_{i}$, 
$\mathbf{z{_{-w{_{i}}}}}$ is the current assignment topics of all $k$-mers except for $w_{i}$, 
$n^{w_{i}}_{-i,j}$ is the total number of $k$-mers assigned to latent topic $j$ except for the current $k$-mer $w_{i}$, 
$n^{d}_{-i,j}$ is the total number of $k$-mers except for $w_{i}$ in read $d$ that have been assigned to topic $j$, 
$n^{*}_{-i,j}$ is the total number of $k$-mers except for $w_{i}$ assigned to latent topic $j$, 
and $n^{d}_{-i,*}$ is the total number of $k$-mers except for $w_{i}$ in read $d$.


在我们的模型中，我们设定Dirichlet先验的参数$\alpha$=0.1,  $\beta$=0.01, 这样的参数设定是为了使得主题模型的结果多种多样的。


\section{本章小结}
在本章中，我们介绍了机器学习的一类非常重要及流行的主题模型，以LDA作为代表模型，简单介绍了LDA的来龙去脉，如何提出LDA模型，如何去求解，等等。在文档处理中，LDA因为可以抽取文档的主题信息，提取语义信息而受到广泛的关注。最初由于Variation Bayesian的方法求解

